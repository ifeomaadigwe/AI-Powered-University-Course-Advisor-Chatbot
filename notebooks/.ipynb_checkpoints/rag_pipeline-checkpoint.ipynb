{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5202b1f-3d8d-4915-a479-e024ea007c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IfeomaAugustaAdigwe\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# # ----- RAG Pipeline Setup -----\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e586dab-795c-46b5-8511-7f376ab0cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ifeomaaugustaadigwe\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b523906-2a8c-4fb5-865d-9bce8cf7741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:27<00:00, 13.55s/it]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=f\"logs/chat_{datetime.now().strftime('%Y-%m-%d')}.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Load FAISS index and documents\n",
    "index = faiss.read_index(\"docs/lmu_index.faiss\")\n",
    "with open(\"docs/lmu_documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# Load embedding model (improved retrieval)\n",
    "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
    "\n",
    "# Load language model\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create generation pipeline\n",
    "qa_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def get_answer(query: str) -> str:\n",
    "    # Encode and normalize the query for better search accuracy\n",
    "    query_embedding = embedder.encode([query], normalize_embeddings=True)\n",
    "    k = 5  # number of relevant chunks to retrieve\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "    # Extract and clean context chunks\n",
    "    seen = set()\n",
    "    context_parts = []\n",
    "    for i in indices[0]:\n",
    "        text = documents[i][\"text\"]\n",
    "        text = text.replace(\"â\", \"'\").replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        truncated = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(truncated) + len(sentence) <= 500:\n",
    "                truncated += sentence + \" \"\n",
    "            else:\n",
    "                break\n",
    "        cleaned = truncated.strip()\n",
    "        if cleaned not in seen:\n",
    "            seen.add(cleaned)\n",
    "            context_parts.append(cleaned)\n",
    "\n",
    "    context = \"\\n\".join(context_parts)\n",
    "\n",
    "    # Build the prompt with safety guardrails\n",
    "    prompt = f\"\"\"You are a helpful academic advisor for LMU Munich. Use ONLY the context below to answer the user's question. \n",
    "If the answer is not clearly in the context, say \"I’m not sure, please refer to LMU’s official website.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Generate the response\n",
    "    response = qa_pipeline(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
    "    answer = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    # Logging\n",
    "    logging.info(f\"User Query: {query}\")\n",
    "    logging.info(f\"Context Used: {context}\")\n",
    "    logging.info(f\"Generated Answer: {answer}\")\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abeebc-e764-427a-af9c-9f7df62c6d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806c848-6d50-430c-98c6-d0959366fe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
